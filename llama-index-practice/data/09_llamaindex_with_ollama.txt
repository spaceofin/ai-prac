LlamaIndex는 Ollama와 함께 사용하면 로컬 환경에서 LLM을 실행할 수 있습니다.  
이 구조는 인터넷 없이도 작동하며, 클라우드 API 없이 완전한 자율성을 가집니다.  
`@llamaindex/ollama` 패키지를 설치한 후 모델을 지정하면 됩니다.  
예: mixtral, llama2 등의 모델을 사용할 수 있습니다.  
로컬 문서를 임베딩하고 인덱싱한 후 바로 쿼리할 수 있습니다.  
개인 정보 보호나 비용 효율성이 중요한 프로젝트에 적합합니다.  
Ollama 서버가 로컬에서 실행 중이어야 합니다.  
기본적으로 http://localhost:11434 에서 요청을 받습니다.  
LlamaIndex는 내부적으로 이 서버에 요청을 전송합니다.  
OpenAI를 대체할 수 있는 훌륭한 선택지입니다.
